# -*- coding: utf-8 -*-
"""UnetModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FXtn4yWB2O5JqqPyg399lfMEqNAeBaX8
"""

from __future__ import division
import torch.nn as nn
import torch.nn.functional as F
import torch
from numpy.linalg import svd
from numpy.random import normal
from math import sqrt
from tqdm import tqdm
import time


# !/opt/bin/nvidia-smi

class UNet(nn.Module):
  def __init__(self,colordim =1):
        super(UNet, self).__init__()
        self.dropout = nn.Dropout(p = 0.1)
        #self.bn0 = nn.BatchNorm2d(3)
        self.conv1_1 = nn.Conv2d(3, 32, 3, padding = 1, padding_mode = 'replicate')  # input of (n,n,3), output of (n,n,64)
        self.conv1_2 = nn.Conv2d(32, 32, 3, padding = 1, padding_mode = 'replicate')
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2_1 = nn.Conv2d(32, 64, 3, padding = 1, padding_mode = 'replicate')
        self.conv2_2 = nn.Conv2d(64, 64, 3, padding = 1, padding_mode = 'replicate')
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3_1 = nn.Conv2d(64, 128, 3, padding = 1, padding_mode = 'replicate')
        self.conv3_2 = nn.Conv2d(128, 128, 3, padding = 1, padding_mode = 'replicate')
        self.bn3 = nn.BatchNorm2d(128)
        self.conv4_1 = nn.Conv2d(128, 256, 3, padding = 1, padding_mode = 'replicate')
        self.conv4_2 = nn.Conv2d(256, 256, 3, padding = 1, padding_mode = 'replicate')
        self.bn4 = nn.BatchNorm2d(256)
        self.conv5_1 = nn.Conv2d(256, 512, 3, padding = 1, padding_mode = 'replicate')
        self.conv5_2 = nn.Conv2d(512, 512, 3, padding = 1, padding_mode = 'replicate')
        #self.upconv5 = nn.Conv2d(1024, 512, 1)
        self.upconv5 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.bn5 = nn.BatchNorm2d(256)
        self.bn5_out = nn.BatchNorm2d(512)
        self.conv6_1 = nn.Conv2d(512, 256, 3, padding = 1, padding_mode = 'replicate')
        self.conv6_2 = nn.Conv2d(256, 256, 3, padding = 1, padding_mode = 'replicate')
        #self.upconv6 = nn.Conv2d(512, 256, 1)
        self.upconv6 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.bn6 = nn.BatchNorm2d(128)
        self.bn6_out = nn.BatchNorm2d(256)
        self.conv7_1 = nn.Conv2d(256, 128, 3, padding = 1, padding_mode = 'replicate')
        self.conv7_2 = nn.Conv2d(128, 128, 3, padding = 1, padding_mode = 'replicate')
        #self.upconv7 = nn.Conv2d(256, 128, 1)
        self.upconv7 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.bn7 = nn.BatchNorm2d(64)
        self.bn7_out = nn.BatchNorm2d(128)
        self.conv8_1 = nn.Conv2d(128, 64, 3, padding = 1, padding_mode = 'replicate')
        self.conv8_2 = nn.Conv2d(64, 64, 3, padding = 1, padding_mode = 'replicate')
        #self.upconv8 = nn.Conv2d(128, 64, 1)
        self.upconv8 = nn.ConvTranspose2d(64, 32, 2, stride=2)
        self.bn8 = nn.BatchNorm2d(32)
        self.bn8_out = nn.BatchNorm2d(64)
        self.conv9_1 = nn.Conv2d(64, 32, 3, padding = 1, padding_mode = 'replicate')
        self.conv9_2 = nn.Conv2d(32, 32, 3, padding = 1, padding_mode = 'replicate')
        self.conv9_3 = nn.Conv2d(32, colordim, 1)
        self.bn9 = nn.BatchNorm2d(colordim)
        self.bn10 = nn.BatchNorm2d(32)
        self.bn11 = nn.BatchNorm2d(32)
        self.bn12 = nn.BatchNorm2d(1)
        self.maxpool = nn.MaxPool2d(2, stride=2, return_indices=False, ceil_mode=False)
        #self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)

  def forward(self, x1):
        x1 = F.relu(self.bn1(self.conv1_2(F.relu(self.conv1_1(x1)))))
        #self.feature1 = x1
        # print('x1 size: %d'%(x1.size(2)))
        #print('x1: %d%d'%(torch.max(x1),torch.min(x1)))
        x2 = F.relu(self.bn2(self.conv2_2(F.relu(self.conv2_1(self.maxpool(x1))))))
        #self.feature2 = x2
        # print('x2 size: %d'%(x2.size(2)))
        #print('x2: %d%d'%(torch.max(x2),torch.min(x2)))
        x3 = F.relu(self.bn3(self.conv3_2(F.relu(self.conv3_1(self.maxpool(x2))))))
        #self.feature3 = x3
        # print('x3 size: %d'%(x3.size(2)))
        #print('x3: %d%d'%(torch.max(x3),torch.min(x3)))
        x4 = F.relu(self.bn4(self.conv4_2(F.relu(self.conv4_1(self.maxpool(x3))))))
        #self.feature4 = x4
        # print('x4 size: %d'%(x4.size(2)))
        #print('x4: %d%d'%(torch.max(x4),torch.min(x4)))
        xup = F.relu(self.conv5_2(F.relu(self.conv5_1(self.maxpool(x4)))))  # x5
        # print('x5 size: %d'%(xup.size(2)))
        
        xup = F.relu(self.conv6_2(F.relu(self.bn5(self.upconv5(xup)))))
        
        xup = F.relu(self.conv7_2(F.relu(self.bn6(self.upconv6(xup)))))
        
        xup = F.relu(self.conv8_2(F.relu(self.bn7(self.upconv7(xup)))))
        
        xup = F.relu(self.conv9_3(F.relu(self.bn8(self.upconv8(xup)))))

        xup = torch.sigmoid(self.bn12(xup))
        #print(xup[0,0,30,30])
        #xup = xup.view(32*128*128)
        #xup = F.softmax(xup,dim = 0)
        return xup

def main():
    unet = UNet()

    num_epochs = 100  # The number of times entire dataset is trained
    batch_size = 128  # The size of input data took for one iteration
    # learning_rate = 0.001

    import torch.optim as optim

    # device = torch.device('cuda')

    # def criterion(outputs,labels):
    #   num_correct = 0
    #   outputs = outputs.cpu().data.numpy().squeeze()
    #   labels = labels.cpu().data.numpy().squeeze()
    #   dic = {}
    #   # print(outputs.shape)
    #   for i in range(np.size(outputs,0)):
    #     num_correct = 0
    #     for j in range(np.size(outputs,1)):
    #       for k in range(np.size(outputs,2)):
    #         if(outputs[i,j,k]>=0.5):
    #           outputs[i,j,k]=1
    #         else:
    #           outputs[i,j,k]=0
    #         if(outputs[i,j,k]==labels[i,j,k]):
    #           num_correct+=1

    #     dic[i]=1-num_correct/(np.size(outputs,1)*np.size(outputs,2))

    #   loss = np.array(list(dic.values()))
    #   print(loss)
    #   return torch.from_numpy(loss)
    criterion = nn.BCELoss()

    optimizer = optim.Adam(unet.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80, 150, 200, 250, 300, 350, 400, 450])
    # unet.to(device)

    # x = torch.rand(size=(8, 3, 128, 128))
    # from torchsummary import summary
    # summary(unet, (3, 128, 128))

    # from google.colab import drive
    # drive.mount('/content/drive')

    # %cd drive/MyDrive/199/winter2021/

    # !ls

    import numpy as np
    import pandas as pd
    # import tensorflow as tf
    from PIL import Image
    from matplotlib import pyplot as plt

    X_train = np.load('X_train_aug.npy') / 255 - 0.5
    # print(np.max(X_train))
    Y_train = np.load('Y_train_aug.npy')
    # X_test = np.load('X_test.npy')
    # Y_test = np.load('Y_test.npy')

    X_train = X_train.transpose((0, 3, 1, 2))
    Y_train = Y_train.transpose((0, 3, 1, 2))
    # X_test = X_test.transpose((0,3,1,2))
    # Y_test = Y_test.transpose((0,3,1,2))
    # Y_train = np.squeeze(Y_train)
    # Y_train = Y_train.view((16384, 128*128))
    # int(Y_train.shape)
    import torch.utils.data as Data

    torch_dataset = Data.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(Y_train).float())
    loader = Data.DataLoader(
        dataset=torch_dataset,
        batch_size=batch_size,
        shuffle=True)
    # test_dataset = Data.TensorDataset(torch.from_numpy(X_test).float(),torch.from_numpy(Y_test).float())
    # test_loader = Data.DataLoader(
    #       dataset = test_dataset,
    #      batch_size =
    #     shuffle = False)
    del X_train
    del Y_train


    def IOU(output, label):
        output[output >= 0.5] = 1
        output[output < 0.5] = 0
        iou = torch.sum(output[output == label])

        IOU = iou / (torch.sum(output) + torch.sum(label) - iou + 1e-6)
        return IOU.item()


    import os

    os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1, 2'
    unet = nn.DataParallel(unet, device_ids=[0, 1, 2], output_device=0)
    unet = unet.cuda(0)
    unet.train()


    correct_dict={}
    loss_dict={}
    for epoch in tqdm(range(num_epochs)):

        running_loss = 0.
        batch_size = batch_size
        list_loss = []
        iou_list = []
        for step, (batch_x, batch_y) in enumerate(tqdm(loader)):
            inputs, labels = batch_x, batch_y
            inputs, labels = inputs.cuda(0), labels.cuda(0)
            inputs, labels = inputs.to(dtype=torch.float), labels.to(dtype=torch.float)
            # print(torch.max(inputs))
            optimizer.zero_grad()
            # print(next(unet.parameters()).device)
            # print(inputs.device)
            start = time.time()
            outputs = unet(inputs)
            # print("forward time", time.time()-start)
            # outputs = data_norm(outputs)
            # outputs = outputs.cuda()
            # outputs=outputs.squeeze();
            # labels=labels.squeeze();
            # print(outputs)
            # print(labels)
            # print(outputs)
            loss = criterion(outputs, labels)
            # print(loss)
            s = time.time()
            loss.backward()
            # print("back time", time.time()-s)
            optimizer.step()
            iou_list.append(IOU(outputs, labels))
            # print('[%d, %5d]' %(epoch + 1, (step+1)*batch_size))
            # print(loss.item())
            list_loss.append(loss.cpu().item())
            scheduler.step()
        loss_dict[epoch]=(sum(list_loss) / len(list_loss))
        correct_dict[epoch] = (sum(iou_list) / len(iou_list))
        # break
        torch.save(unet.state_dict(), 'model_FCN/new_%s.pt' % (epoch))

    plt.title('IOU train data')
    plt.xlabel('epoch')
    plt.ylabel('IOU')
    plt.plot(range(100), correct_dict.values(), label='IOU')
    plt.legend()
    plt.savefig('IOUtraining_FCN.jpg')
    plt.clf()
    # max_key = max(correct_dict, key=correct_dict.get)
    # print('The largest correctness is model No. '+str(max_key)+', and the rate is '+str(correct_dict[max_key]))
    plt.title('loss train data')
    plt.plot(range(100), loss_dict.values(), label='loss')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.legend()
    plt.savefig('Losstrain_FCN.jpg')

    print('Finished Training')

if __name__ == '__main__':
    main()