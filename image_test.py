# -*- coding: utf-8 -*-
"""image_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wq8zaUVBi2eehHi1d6VpV5THlnLNf4Nf
"""

import torch
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt

class UNet(nn.Module):
  def __init__(self,colordim =1):
        super(UNet, self).__init__()
        self.dropout = nn.Dropout(p = 0.5)
        #self.bn0 = nn.BatchNorm2d(3)
        self.conv1_1 = nn.Conv2d(3, 32, 3, padding = 1, padding_mode = 'replicate')  # input of (n,n,3), output of (n,n,64)
        self.conv1_2 = nn.Conv2d(32, 32, 3, padding = 1, padding_mode = 'replicate')
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2_1 = nn.Conv2d(32, 64, 3, padding = 1, padding_mode = 'replicate')
        self.conv2_2 = nn.Conv2d(64, 64, 3, padding = 1, padding_mode = 'replicate')
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3_1 = nn.Conv2d(64, 128, 3, padding = 1, padding_mode = 'replicate')
        self.conv3_2 = nn.Conv2d(128, 128, 3, padding = 1, padding_mode = 'replicate')
        self.bn3 = nn.BatchNorm2d(128)
        self.conv4_1 = nn.Conv2d(128, 256, 3, padding = 1, padding_mode = 'replicate')
        self.conv4_2 = nn.Conv2d(256, 256, 3, padding = 1, padding_mode = 'replicate')
        self.bn4 = nn.BatchNorm2d(256)
        self.conv5_1 = nn.Conv2d(256, 512, 3, padding = 1, padding_mode = 'replicate')
        self.conv5_2 = nn.Conv2d(512, 512, 3, padding = 1, padding_mode = 'replicate')
        #self.upconv5 = nn.Conv2d(1024, 512, 1)
        self.upconv5 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.bn5 = nn.BatchNorm2d(256)
        self.bn5_out = nn.BatchNorm2d(512)
        self.conv6_1 = nn.Conv2d(512, 256, 3, padding = 1, padding_mode = 'replicate')
        self.conv6_2 = nn.Conv2d(256, 256, 3, padding = 1, padding_mode = 'replicate')
        #self.upconv6 = nn.Conv2d(512, 256, 1)
        self.upconv6 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.bn6 = nn.BatchNorm2d(128)
        self.bn6_out = nn.BatchNorm2d(256)
        self.conv7_1 = nn.Conv2d(256, 128, 3, padding = 1, padding_mode = 'replicate')
        self.conv7_2 = nn.Conv2d(128, 128, 3, padding = 1, padding_mode = 'replicate')
        #self.upconv7 = nn.Conv2d(256, 128, 1)
        self.upconv7 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.bn7 = nn.BatchNorm2d(64)
        self.bn7_out = nn.BatchNorm2d(128)
        self.conv8_1 = nn.Conv2d(128, 64, 3, padding = 1, padding_mode = 'replicate')
        self.conv8_2 = nn.Conv2d(64, 64, 3, padding = 1, padding_mode = 'replicate')
        #self.upconv8 = nn.Conv2d(128, 64, 1)
        self.upconv8 = nn.ConvTranspose2d(64, 32, 2, stride=2)
        self.bn8 = nn.BatchNorm2d(32)
        self.bn8_out = nn.BatchNorm2d(64)
        self.conv9_1 = nn.Conv2d(64, 32, 3, padding = 1, padding_mode = 'replicate')
        self.conv9_2 = nn.Conv2d(32, 32, 3, padding = 1, padding_mode = 'replicate')
        self.conv9_3 = nn.Conv2d(32, colordim, 1)
        self.bn9 = nn.BatchNorm2d(colordim)
        self.bn10 = nn.BatchNorm2d(32)
        self.bn11 = nn.BatchNorm2d(32)
        self.bn12 = nn.BatchNorm2d(1)
        self.maxpool = nn.MaxPool2d(2, stride=2, return_indices=False, ceil_mode=False)
        #self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)

  def forward(self, x1):
        x1 = F.relu(self.bn1(self.conv1_2(F.relu(self.conv1_1(x1)))))
        #self.feature1 = x1
        # print('x1 size: %d'%(x1.size(2)))
        #print('x1: %d%d'%(torch.max(x1),torch.min(x1)))
        x2 = F.relu(self.bn2(self.conv2_2(F.relu(self.conv2_1(self.maxpool(x1))))))
        #self.feature2 = x2
        # print('x2 size: %d'%(x2.size(2)))
        #print('x2: %d%d'%(torch.max(x2),torch.min(x2)))
        x3 = F.relu(self.bn3(self.conv3_2(F.relu(self.conv3_1(self.maxpool(x2))))))
        #self.feature3 = x3
        # print('x3 size: %d'%(x3.size(2)))
        #print('x3: %d%d'%(torch.max(x3),torch.min(x3)))
        x4 = F.relu(self.bn4(self.conv4_2(F.relu(self.conv4_1(self.maxpool(x3))))))
        #self.feature4 = x4
        # print('x4 size: %d'%(x4.size(2)))
        #print('x4: %d%d'%(torch.max(x4),torch.min(x4)))
        xup = F.relu(self.conv5_2(F.relu(self.conv5_1(self.maxpool(x4)))))  # x5
        # print('x5 size: %d'%(xup.size(2)))

        xup = self.dropout(self.bn5(self.upconv5(xup)))
        # print('x6 size: %d'%(xup.size(2)))
        cropidx = (x4.size(2) - xup.size(2)) // 2
        x4 = x4[:, :, cropidx:cropidx + xup.size(2), cropidx:cropidx + xup.size(2)]
        # print('crop1 size: %d, x9 size: %d'%(x4.size(2),xup.size(2)))
        xup = self.bn5_out(torch.cat((x4, xup), 1))  # x6 cat x4
        xup = F.relu(self.conv6_2(F.relu(self.conv6_1(xup))))  # x6out

        xup = self.dropout(self.bn6(self.upconv6(xup)))# x7in
        #print('xup1: %d%d'%(torch.max(xup),torch.min(xup)))
        cropidx = (x3.size(2) - xup.size(2)) // 2
        x3 = x3[:, :, cropidx:cropidx + xup.size(2), cropidx:cropidx + xup.size(2)]
        # print('crop1 size: %d, x9 size: %d'%(x3.size(2),xup.size(2)))
        xup = self.bn6_out(torch.cat((x3, xup), 1) ) # x7 cat x3
        xup = F.relu(self.conv7_2(F.relu(self.conv7_1(xup))))  # x7out

        xup = self.dropout(self.bn7(self.upconv7(xup))) # x8in
        #print('xup2: %d%d'%(torch.max(xup),torch.min(xup)))
        cropidx = (x2.size(2) - xup.size(2)) // 2
        x2 = x2[:, :, cropidx:cropidx + xup.size(2), cropidx:cropidx + xup.size(2)]
        # print('crop1 size: %d, x9 size: %d'%(x2.size(2),xup.size(2)))
        xup = self.bn7_out(torch.cat((x2, xup), 1))  # x8 cat x2
        xup = F.relu(self.conv8_2(F.relu(self.conv8_1(xup))))  # x8out

        xup = self.dropout(self.bn8(self.upconv8(xup))) # x9in
        #print('xup3: %d%d'%(torch.max(xup),torch.min(xup)))
        cropidx = (x1.size(2) - xup.size(2)) // 2
        x1 = x1[:, :, cropidx:cropidx + xup.size(2), cropidx:cropidx + xup.size(2)]
        # print('crop1 size: %d, x9 size: %d'%(x1.size(2),xup.size(2)))
        xup = self.bn8_out(torch.cat((x1, xup), 1))  # x9 cat x1
        #print('xup4.1:%d,%d'%(torch.max(xup),torch.min(xup)))
        xup = F.relu(self.bn10((self.conv9_1(xup))))
        #print('xup4.2:%d,%d'%(torch.max(xup),torch.min(xup)))
        xup = F.relu(self.bn11((self.conv9_2(xup))))
        #print('xup4.3:%d,%d'%(torch.max(xup),torch.min(xup)))
        xup = self.conv9_3(xup)
        #print('xup4.4:%d,%d'%(torch.max(xup),torch.min(xup)))
        #xup = self.conv9_3(F.relu(self.conv9_2(F.relu(self.conv9_1(xup)))))  # x9out
        #print(xup.shape)
        #print('xup4: %d,%d'%(torch.min(xup),torch.max(xup)))
        #print(xup[0,0,30,30])
        xup = torch.sigmoid(self.bn12(xup))
        #print(xup[0,0,30,30])
        #xup = xup.view(32*128*128)
        #xup = F.softmax(xup,dim = 0)
        return xup

X_test=np.load('X_test.npy')
Y_test=np.load('Y_test.npy')

# X_train=np.load('X_train_aug.npy')
# Y_train=np.load('Y_train_aug.npy')

X_test = X_test.transpose((0,3,1,2))
Y_test = Y_test.transpose((0,3,1,2))

import torch.utils.data as Data
batch_size=64
torch_dataset = Data.TensorDataset(torch.from_numpy(X_test).float(),torch.from_numpy(Y_test).float())
loader = Data.DataLoader(
        dataset = torch_dataset,
        batch_size = batch_size)

del X_test
del Y_test

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
loss = nn.BCELoss()
def criterion(outputs,labels):
    outputs[outputs>=0.5] = 1
    outputs[outputs<0.5] = 0
    iou = torch.sum(outputs[outputs == labels])

    IOU = iou/(torch.sum(outputs)+torch.sum(labels)-iou+1e-6)
    return IOU.item()
          

# correct_dict={}
# loss_dict={}
# for i in tqdm(range(100)):
#   correct_data = []
#   loss_data = []
#   unet = UNet()
#   unet = nn.DataParallel(unet, device_ids = [0], output_device  = 0)
#   unet = unet.cuda(0)
#   # unet.load_state_dict(torch.load('model/%s.pkl' % (i+1)))
#   unet.load_state_dict(torch.load('model/%s.pt' % (i)))
#   for x, y in tqdm(loader):
#       x, y = x.cuda(0), y.cuda(0)
#       with torch.no_grad():
#         predict = unet(x)
#       correct = criterion(predict,y)
#       loss_item = loss(predict, y).cpu()
#       correct_data.append(correct)
#       loss_data.append(loss_item)
#   correct_dict[i] = sum(correct_data)/len(correct_data)
#   loss_dict[i] = sum(loss_data) / len(loss_data)
#   # print(correct_dict[i])

# plt.title('IOU test data')
# plt.xlabel('epoch')
# plt.ylabel('IOU')
# plt.plot(range(100), correct_dict.values(), label='IOU')
# plt.legend()
# plt.savefig('IOUtesting.jpg')
# plt.clf()
# # max_key = max(correct_dict, key=correct_dict.get)
# # print('The largest correctness is model No. '+str(max_key)+', and the rate is '+str(correct_dict[max_key]))
# plt.title('loss test data')
# plt.plot(range(100), loss_dict.values(), label='loss')
# plt.xlabel('epoch')
# plt.ylabel('loss')
# plt.legend()
# plt.savefig('Losstesting.jpg')
# plt.clf()


X_test=np.load('X_train_aug.npy')
Y_test=np.load('Y_train_aug.npy')

X_test = X_test.transpose((0,3,1,2))
Y_test = Y_test.transpose((0,3,1,2))

torch_dataset = Data.TensorDataset(torch.from_numpy(X_test).float(),torch.from_numpy(Y_test).float())
loader = Data.DataLoader(
        dataset = torch_dataset,
        batch_size = batch_size)

correct_dict={}
loss_dict={}
for i in tqdm(range(100)):
  correct_data = []
  loss_data = []
  unet = UNet()
  unet = nn.DataParallel(unet, device_ids = [0], output_device  = 0)
  unet = unet.cuda(0)
  # unet.load_state_dict(torch.load('model/%s.pkl' % (i+1)))
  unet.load_state_dict(torch.load('model/%s.pt' % (i)))
  for x, y in tqdm(loader):
      x, y = x.cuda(0), y.cuda(0)
      with torch.no_grad():
        predict = unet(x)
      correct = criterion(predict,y)
      loss_item = loss(predict, y).cpu()
      correct_data.append(correct)
      loss_data.append(loss_item)
  correct_dict[i] = sum(correct_data)/len(correct_data)
  loss_dict[i] = sum(loss_data) / len(loss_data)

plt.title('IOU train data')
plt.xlabel('epoch')
plt.ylabel('IOU')
plt.plot(range(100), correct_dict.values(), label='IOU')
plt.legend()
plt.savefig('IOUtraining.jpg')
# plt.clf()
# # max_key = max(correct_dict, key=correct_dict.get)
# # print('The largest correctness is model No. '+str(max_key)+', and the rate is '+str(correct_dict[max_key]))
# plt.title('loss train data')
# plt.plot(range(100), loss_dict.values(), label='loss')
# plt.xlabel('epoch')
# plt.ylabel('loss')
# plt.legend()
# plt.savefig('Losstrain.jpg')